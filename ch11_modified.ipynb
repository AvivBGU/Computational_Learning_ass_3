{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGES ####\n",
    "Moved imports to a dedicated cell.\n",
    "Moved magic nubmers to a dedicated constants cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THESE ARE CONSTANTS\n",
    "RANDOM_SEED: int = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for part 1, An additional layer was added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for index, val in enumerate(y):\n",
    "        ary[index, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    z_deriv = np.array(z, copy=True)\n",
    "    z_deriv[z_deriv <= 0] = 0\n",
    "    z_deriv[z_deriv > 0] = 1\n",
    "    return z_deriv.astype(float)\n",
    "\n",
    "class NeuralNetMLP_2_HiddenLayers:\n",
    "    def __init__(self, \n",
    "                 num_features: int, \n",
    "                 hidden_layer1_size: int, \n",
    "                 hidden_layer2_size: int,\n",
    "                 num_classes: int, \n",
    "                 random_seed=RANDOM_SEED\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # hidden\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        \n",
    "        self.weight_hidden_layer_1 = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(hidden_layer1_size, num_features))\n",
    "        self.bias_h1 = np.zeros(hidden_layer1_size)\n",
    "\n",
    "        self.weight_hidden_layer_2 = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(hidden_layer2_size, hidden_layer1_size))\n",
    "        self.bias_h2 = np.zeros(hidden_layer2_size)\n",
    "\n",
    "        # output\n",
    "        self.weight_out = rng.normal(\n",
    "            loc=0.0, scale=0.1, size=(num_classes, hidden_layer2_size))\n",
    "        self.bias_out = np.zeros(num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Hidden layer 1\n",
    "        # input dim: [n_examples, n_features] dot [n_hidden1, n_features].T\n",
    "        z_hidden_layer_1 = np.dot(x, self.weight_hidden_layer_1.T) + self.bias_h1\n",
    "        a_hidden_layer_1 = sigmoid(z_hidden_layer_1)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        # input dim: [n_examples, n_hidden1] dot [n_hidden2, n_hidden1].T\n",
    "        z_hidden_layer_2 = np.dot(a_hidden_layer_1, self.weight_hidden_layer_2.T) + self.bias_h2\n",
    "        a_hidden_layer_2 = sigmoid(z_hidden_layer_2)\n",
    "\n",
    "        # Output layer\n",
    "        # input dim: [n_examples, n_hidden2] dot [n_classes, n_hidden2].T\n",
    "        # output dim: [n_examples, n_classes]\n",
    "        z_out = np.dot(a_hidden_layer_2, self.weight_out.T) + self.bias_out\n",
    "        a_out = sigmoid(z_out)\n",
    "        return a_hidden_layer_1, a_hidden_layer_2, a_out\n",
    "\n",
    "    def backward(self, x, a_hidden_layer_1, a_hidden_layer_2, a_out, y):\n",
    "\n",
    "        # onehot encoding\n",
    "        y_onehot = int_to_onehot(y, self.num_classes)\n",
    "\n",
    "        #########################\n",
    "        # Output layer gradients\n",
    "        #########################\n",
    "        d_loss__d_a_out = 2.0 * (a_out - y_onehot) / y.shape[0]\n",
    "        d_a_out__d_z_out = a_out * (1.0 - a_out)             # sigmoid'\n",
    "        delta_out = d_loss__d_a_out * d_a_out__d_z_out       # (N, C)\n",
    "\n",
    "        # W_out: (C, H2), b_out: (C,)\n",
    "        d_loss__dw_out = np.dot(delta_out.T, a_hidden_layer_2)   # (C, H2)\n",
    "        d_loss__db_out = np.sum(delta_out, axis=0)               # (C,)\n",
    "\n",
    "        ########################################\n",
    "        # Hidden layer 2 gradients (sigmoid)\n",
    "        ########################################\n",
    "        # backprop into a_hidden_layer_2: (N, H2)\n",
    "        d_loss__d_a2 = np.dot(delta_out, self.weight_out)        # (N, H2)\n",
    "        d_a2__d_z2 = a_hidden_layer_2 * (1.0 - a_hidden_layer_2) # sigmoid'\n",
    "        delta_2 = d_loss__d_a2 * d_a2__d_z2                      # (N, H2)\n",
    "\n",
    "        # W2: (H2, H1), b2: (H2,)\n",
    "        d_loss__dw_h2 = np.dot(delta_2.T, a_hidden_layer_1)      # (H2, H1)\n",
    "        d_loss__db_h2 = np.sum(delta_2, axis=0)                  # (H2,)\n",
    "\n",
    "        ########################################\n",
    "        # Hidden layer 1 gradients (sigmoid)\n",
    "        ########################################\n",
    "        # backprop into a_hidden_layer_1: (N, H1)\n",
    "        d_loss__d_a1 = np.dot(delta_2, self.weight_hidden_layer_2)   # (N, H1)\n",
    "        d_a1__d_z1 = a_hidden_layer_1 * (1.0 - a_hidden_layer_1)     # sigmoid'\n",
    "        delta_1 = d_loss__d_a1 * d_a1__d_z1                          # (N, H1)\n",
    "\n",
    "        # W1: (H1, F), b1: (H1,)\n",
    "        d_loss__dw_h1 = np.dot(delta_1.T, x)                      # (H1, F)\n",
    "        d_loss__db_h1 = np.sum(delta_1, axis=0)                   # (H1,)\n",
    "\n",
    "        return (d_loss__dw_out, d_loss__db_out,\n",
    "                d_loss__dw_h2,  d_loss__db_h2,\n",
    "                d_loss__dw_h1,  d_loss__db_h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetMLP_2_HiddenLayers(num_features=28*28,\n",
    "                     hidden_layer1_size=50,\n",
    "                     hidden_layer2_size=50,\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "    \n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "\n",
    "_, _, probas = model.forward(X_valid)\n",
    "mse = mse_loss(y_valid, probas)\n",
    "\n",
    "predicted_labels = np.argmax(probas, axis=1)\n",
    "acc = accuracy(y_valid, predicted_labels)\n",
    "\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Forward ####\n",
    "            a1, a2, a_out = model.forward(X_train_mini)\n",
    "\n",
    "            #### Backward ####\n",
    "            (dW_out, db_out,\n",
    "             dW2, db2,\n",
    "             dW1, db1) = model.backward(\n",
    "                X_train_mini,\n",
    "                a1, a2,\n",
    "                a_out,\n",
    "                y_train_mini\n",
    "            )\n",
    "\n",
    "            #### Update ####\n",
    "            model.weight_hidden_layer_1 -= learning_rate * dW1\n",
    "            model.bias_h1              -= learning_rate * db1\n",
    "\n",
    "            model.weight_hidden_layer_2 -= learning_rate * dW2\n",
    "            model.bias_h2              -= learning_rate * db2\n",
    "\n",
    "            model.weight_out -= learning_rate * dW_out\n",
    "            model.bias_out   -= learning_rate * db_out\n",
    "\n",
    "        #### Epoch Logging ####\n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "\n",
    "        train_acc *= 100.0\n",
    "        valid_acc *= 100.0\n",
    "\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED) # for the training set shuffling\n",
    "\n",
    "epoch_loss, epoch_train_acc, epoch_valid_acc = train(\n",
    "    model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=50, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the neural network performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "#plt.savefig('figures/11_07.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n",
    "         label='Training')\n",
    "plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n",
    "         label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "#plt.savefig('figures/11_08.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, test_acc = compute_mse_and_acc(model, X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot failure cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, _, probas = model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_09.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
