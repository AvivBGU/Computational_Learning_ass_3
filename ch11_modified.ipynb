{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGES ####\n",
    "Moved imports to a dedicated cell.\n",
    "Moved magic nubmers to a dedicated constants cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED: int = 123\n",
    "NUM_EPOCHS: int = 50\n",
    "LEARNING_RATE: float = 0.1\n",
    "TRAIN_TEST_SPLIT = 0.3\n",
    "TRAIN_VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=TRAIN_TEST_SPLIT, random_state=RANDOM_SEED, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=TRAIN_VALIDATION_SPLIT, random_state=RANDOM_SEED, stratify=y_temp)\n",
    "\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was changed to be modular to support an arbitrary number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for index, val in enumerate(y):\n",
    "        ary[index, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "class NeuralNetMLP:\n",
    "    def __init__(self, \n",
    "                 num_features: int, \n",
    "                 hidden_layers_sizes: list[int],\n",
    "                 num_classes: int,\n",
    "                 random_seed=RANDOM_SEED\n",
    "                 ):\n",
    "        \n",
    "        \n",
    "        if len(hidden_layers_sizes) < 1:\n",
    "            raise ValueError(\"hidden_layers_sizes must contain at least one layer.\")\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.num_classes = num_classes\n",
    "        self.layer_weights = [rng.normal(loc=0.0, scale=0.1, size=(hidden_layers_sizes[0], num_features))]\n",
    "        self.biases = list()\n",
    "        # hidden\n",
    "        \n",
    "        \n",
    "        for index in range(1, len(hidden_layers_sizes)):\n",
    "            self.layer_weights.append(\n",
    "                rng.normal(\n",
    "                    loc=0.0, \n",
    "                    scale=0.1, \n",
    "                    size=(hidden_layers_sizes[index], hidden_layers_sizes[index-1])\n",
    "                )\n",
    "            )\n",
    "        for index in range(len(hidden_layers_sizes)):\n",
    "            self.biases.append(np.zeros(hidden_layers_sizes[index]))\n",
    "\n",
    "        # output\n",
    "        self.layer_weights.append(rng.normal(loc=0.0, scale=0.1, size=(num_classes, hidden_layers_sizes[-1])))\n",
    "        self.biases.append(np.zeros(num_classes))\n",
    "        \n",
    "    def forward(self, input: np.ndarray):\n",
    "        layer_activations = []\n",
    "        layer_pre_activations = []  # store z for ReLU derivative\n",
    "        current_input = input\n",
    "\n",
    "        num_of_layers = len(self.layer_weights)\n",
    "        for layer_index in range(num_of_layers):\n",
    "            z = np.dot(current_input, self.layer_weights[layer_index].T) + self.biases[layer_index]\n",
    "            layer_pre_activations.append(z)\n",
    "            a = sigmoid(z)\n",
    "            layer_activations.append(a)\n",
    "            current_input = a\n",
    "\n",
    "        return layer_activations, layer_activations[-1]\n",
    "\n",
    "    def backward(self, input, layer_activations, network_output, targets):\n",
    "        N = input.shape[0]\n",
    "        targets_onehot = int_to_onehot(targets, self.num_classes)\n",
    "\n",
    "        num_layers = len(self.layer_weights)\n",
    "        loss_weights_list = [None] * num_layers\n",
    "        loss_biases_list  = [None] * num_layers\n",
    "\n",
    "        # output delta (sigmoid + MSE)\n",
    "        a_out = network_output\n",
    "        d_loss__d_a = 2.0 * (a_out - targets_onehot) / N\n",
    "        delta = d_loss__d_a * (a_out * (1.0 - a_out))\n",
    "\n",
    "        # backprop\n",
    "        for layer_idx in range(num_layers - 1, -1, -1):\n",
    "            a_prev = input if layer_idx == 0 else layer_activations[layer_idx - 1]\n",
    "\n",
    "            loss_weights_list[layer_idx] = np.dot(delta.T, a_prev)\n",
    "            loss_biases_list[layer_idx]  = np.sum(delta, axis=0)\n",
    "\n",
    "            if layer_idx > 0:\n",
    "                curr_W = self.layer_weights[layer_idx]\n",
    "                d_loss__d_a_prev = np.dot(delta, curr_W)\n",
    "\n",
    "                a_prev_act = layer_activations[layer_idx - 1]\n",
    "                delta = d_loss__d_a_prev * (a_prev_act * (1.0 - a_prev_act))\n",
    "\n",
    "        return loss_weights_list, loss_biases_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_model = NeuralNetMLP(num_features=28*28,\n",
    "                     hidden_layers_sizes=[50, 50],\n",
    "                     num_classes=10)\n",
    "single_layer_model = NeuralNetMLP(num_features=28*28,\n",
    "                     hidden_layers_sizes=[50],\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "def evaluate_model(model, features, true_labels):\n",
    "    _, probas = model.forward(features)\n",
    "    mse = mse_loss(true_labels, probas)\n",
    "    predicted_labels = np.argmax(probas, axis=1)\n",
    "    acc = accuracy(true_labels, predicted_labels)\n",
    "    return acc, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, mse = evaluate_model(single_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')\n",
    "acc, mse = evaluate_model(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(single_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')\n",
    "mse, acc = compute_mse_and_acc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(model, X, y):\n",
    "    _, probas = model.forward(X)\n",
    "    y_onehot = int_to_onehot(y, num_labels=model.num_classes)\n",
    "    macro_auc = roc_auc_score(y_onehot, probas, multi_class=\"ovr\", average=\"macro\")\n",
    "    return macro_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_layer_auc = compute_auc(single_layer_model, X_valid, y_valid)\n",
    "double_layer_auc = compute_auc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial AUC single_layer: {single_layer_auc:.5f}')\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial AUC double_layer: {double_layer_auc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=LEARNING_RATE,):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "    epoch_valid_loss = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Forward ####\n",
    "            activations_list, a_out = model.forward(X_train_mini)\n",
    "            #### Backward ####\n",
    "            weight_loss_list, bias_loss_list = model.backward(\n",
    "                X_train_mini,\n",
    "                activations_list,\n",
    "                a_out,\n",
    "                y_train_mini\n",
    "            )\n",
    "\n",
    "            #### Update ####\n",
    "            for layer_idx in range(len(model.layer_weights)):\n",
    "                model.layer_weights[layer_idx] -= learning_rate * weight_loss_list[layer_idx]\n",
    "                model.biases[layer_idx]        -= learning_rate * bias_loss_list[layer_idx]\n",
    "\n",
    "\n",
    "        #### Epoch Logging ####\n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "\n",
    "        train_acc *= 100.0\n",
    "        valid_acc *= 100.0\n",
    "\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "        epoch_valid_loss.append(valid_mse)\n",
    "\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.5f} '\n",
    "              f'| Train Acc: {train_acc:.5f}% '\n",
    "              f'| Valid Acc: {valid_acc:.5f}% '\n",
    "              f'| Valid Loss: {valid_mse:.5f}')\n",
    "    return epoch_loss, epoch_valid_loss, epoch_train_acc, epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED) # for the training set shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training single layer model...')\n",
    "single_layer_epoch_loss, single_layer_epoch_valid_loss, single_layer_epoch_train_acc, single_layer_epoch_valid_acc = train(\n",
    "    single_layer_model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training double layer model...')\n",
    "double_layer_epoch_loss, double_layer_epoch_valid_loss,double_layer_epoch_train_acc, double_layer_epoch_valid_acc = train(\n",
    "    multi_layer_model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the neural network performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(single_layer_epoch_loss)), single_layer_epoch_loss, label=\"single-layer-train\")\n",
    "plt.plot(range(len(single_layer_epoch_valid_loss)), single_layer_epoch_valid_loss, label=\"single-layer-valid\")\n",
    "plt.plot(range(len(double_layer_epoch_loss)), double_layer_epoch_loss, label=\"double-layer-train\")\n",
    "plt.plot(range(len(double_layer_epoch_valid_loss)), double_layer_epoch_valid_loss, label=\"double-layer-valid\")\n",
    "plt.ylabel(\"Training mean squared error\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "# single-layer\n",
    "plt.plot(single_layer_epoch_train_acc, label=\"single-layer train\")\n",
    "plt.plot(single_layer_epoch_valid_acc, label=\"single-layer valid\")\n",
    "\n",
    "# double-layer\n",
    "plt.plot(double_layer_epoch_train_acc, label=\"double-layer train\")\n",
    "plt.plot(double_layer_epoch_valid_acc, label=\"double-layer valid\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_layer_test_mse, single_layer_test_acc = compute_mse_and_acc(single_layer_model, X_test, y_test)\n",
    "double_layer_test_mse, double_layer_test_acc = compute_mse_and_acc(multi_layer_model, X_test, y_test)\n",
    "print(f'Test accuracy, single layer: {single_layer_test_acc*100:.2f}%')\n",
    "print(f'Test accuracy, double layer: {double_layer_test_acc*100:.2f}%')\n",
    "print(f'Test error, single layer: {single_layer_test_mse:.5f}')\n",
    "print(f'Test error, double layer: {double_layer_test_mse:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_layer_auc = compute_auc(single_layer_model, X_valid, y_valid)\n",
    "double_layer_auc = compute_auc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'AUC single_layer after training: {single_layer_auc:.5f}')\n",
    "print('Evaluating double layer model...')\n",
    "print(f'AUC double_layer after training: {double_layer_auc:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot failure cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, probas = single_layer_model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_09.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, probas = multi_layer_model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:20px;\">Implementing an ANN according to the lecture notes.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants relating to the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TRAIN = 100\n",
    "BATCH_SIZE_VALIDATION = 100\n",
    "BATCH_SIZE_TEST = 100\n",
    "ANN_LEARNING_RATE = 0.1\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "INPUT_SIZE = HEIGHT * WIDTH\n",
    "NUM_CLASSES = 10\n",
    "NUM_EPOCHS = 50\n",
    "RANDOM_SEED_FOR_ANN = 42\n",
    "HIDDEN_LAYER_SIZES = [500, 500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layer_sizes: list[int], num_classes: int):\n",
    "        super(DenseNet, self).__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        for h_size in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            nn.init.xavier_uniform_(layers[-1].weight)\n",
    "            nn.init.zeros_(layers[-1].bias)\n",
    "            layers.append(nn.Sigmoid())\n",
    "            in_size = h_size\n",
    "        layers.append(nn.Linear(in_size, num_classes))\n",
    "        layers.append(nn.Softmax(dim=1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_dense_model = DenseNet(input_size=INPUT_SIZE,\n",
    "                           hidden_layer_sizes=HIDDEN_LAYER_SIZES,\n",
    "                           num_classes=NUM_CLASSES)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(ann_dense_model.parameters(), lr=ANN_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ann(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    num_classes,\n",
    "):\n",
    "    train_losses, train_accs = [], []\n",
    "    val_losses, val_accs = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # -------- TRAIN --------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for data, target in minibatch_generator(X_train, y_train, minibatch_size=BATCH_SIZE_TRAIN): # Minibatch size is not passed, as in the original code\n",
    "            data = torch.from_numpy(data).float().view(data.shape[0], -1)\n",
    "            target = torch.from_numpy(target).long()\n",
    "            target_onehot = F.one_hot(target, num_classes=num_classes).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)  # model ends with softmax\n",
    "            loss = criterion(output, target_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_train_loss = running_loss / num_batches\n",
    "        epoch_train_acc = 100.0 * correct / total\n",
    "\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data_val, target_val in minibatch_generator(X_val, y_val, minibatch_size=BATCH_SIZE_VALIDATION): # Minibatch size is not passed, as in the original code\n",
    "                data_val = torch.from_numpy(data_val).float().view(data_val.shape[0], -1)\n",
    "                target_val = torch.from_numpy(target_val).long()\n",
    "                target_val_onehot = F.one_hot(target_val, num_classes=num_classes).float()\n",
    "\n",
    "                output_val = model(data_val)\n",
    "                running_loss += criterion(output_val, target_val_onehot).item()\n",
    "                pred_val = output_val.argmax(dim=1)\n",
    "                correct += (pred_val == target_val).sum().item()\n",
    "                total += target_val.size(0)\n",
    "                num_batches += 1\n",
    "\n",
    "        epoch_val_loss = running_loss / num_batches\n",
    "        epoch_val_acc = 100.0 * correct / total\n",
    "\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1:02d} | \"\n",
    "            f\"train loss {epoch_train_loss:.4f} | \"\n",
    "            f\"train acc {epoch_train_acc:.2f}% | \"\n",
    "            f\"val loss {epoch_val_loss:.4f} | \"\n",
    "            f\"val acc {epoch_val_acc:.2f}%\"\n",
    "        )\n",
    "\n",
    "    return train_losses, train_accs, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accs, val_losses, val_accs = train_ann(ann_dense_model, X_train, y_train, X_valid, y_valid, optimizer, criterion, NUM_EPOCHS, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_accs, label=\"training-accuracy\")\n",
    "plt.plot(range(len(val_accs)), val_accs, label=\"validation-accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(train_losses)), train_losses, label=\"training-loss\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"validation-loss\")\n",
    "plt.ylabel(\"Training mean squared error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 4: evaluating all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_network(\n",
    "    model,\n",
    "    X_test, y_test,\n",
    "    criterion,\n",
    "    num_classes=NUM_CLASSES\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    targets_list = []\n",
    "    outputs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in minibatch_generator(X_test, y_test, minibatch_size=BATCH_SIZE_TEST):\n",
    "            data = torch.from_numpy(data).float().view(data.shape[0], -1)\n",
    "            target = torch.from_numpy(target).long()\n",
    "            output = model(data)\n",
    "\n",
    "            target_onehot = F.one_hot(target, num_classes=num_classes).float()\n",
    "            test_loss += criterion(output, target_onehot).item()\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            num_batches += 1\n",
    "\n",
    "            targets_list.append(target.cpu().numpy())\n",
    "            outputs_list.append(output.cpu().numpy())\n",
    "\n",
    "    y_true = np.concatenate(targets_list, axis=0)\n",
    "    y_score = np.concatenate(outputs_list, axis=0).astype(float)\n",
    "\n",
    "    y_onehot = label_binarize(y_true, classes=np.arange(num_classes))\n",
    "    per_class_auc = [\n",
    "        roc_auc_score(y_onehot[:, c], y_score[:, c])\n",
    "        for c in range(num_classes)\n",
    "    ]\n",
    "    macro_auc = roc_auc_score(\n",
    "        y_onehot, y_score, multi_class=\"ovr\", average=\"macro\"\n",
    "    )\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = correct / total\n",
    "\n",
    "    return test_loss, test_acc, macro_auc, per_class_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ann_loss, test_ann_acc, macro_ann_auc, per_class_auc = evaluate_network(ann_dense_model, X_test, y_test, criterion=criterion, num_classes=NUM_CLASSES)\n",
    "print(\"Evaluating ANN dense model...\")\n",
    "print(f'Test MSE: {test_ann_loss:.5f}')\n",
    "print(f'Test accuracy: {test_ann_acc*100:.5f}%')\n",
    "print(f'Test AUC: {macro_ann_auc:.5f}')\n",
    "print('')\n",
    "single_layer_mse, single_layer_acc = compute_mse_and_acc(single_layer_model, X_valid, y_valid)\n",
    "single_layer_auc = compute_auc(single_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Test MSE: {single_layer_mse:.5f}')\n",
    "print(f'Test accuracy: {single_layer_acc*100:.5f}%')\n",
    "print(f'Test AUC: {single_layer_auc:.5f}')\n",
    "print('')\n",
    "double_layer_mse, double_layer_acc = compute_mse_and_acc(multi_layer_model, X_valid, y_valid)\n",
    "double_layer_auc = compute_auc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Test MSE: {double_layer_mse:.5f}')\n",
    "print(f'Test accuracy: {double_layer_acc*100:.5f}%')\n",
    "print(f'Test AUC: {double_layer_auc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(train_losses)), train_losses, label=\"ann-training-loss\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"ann-validation-loss\")\n",
    "plt.plot(range(len(single_layer_epoch_loss)), single_layer_epoch_loss, label=\"single-layer-train\")\n",
    "plt.plot(range(len(single_layer_epoch_valid_loss)), single_layer_epoch_valid_loss, label=\"single-layer-valid\")\n",
    "plt.plot(range(len(double_layer_epoch_loss)), double_layer_epoch_loss, label=\"double-layer-train\")\n",
    "plt.plot(range(len(double_layer_epoch_valid_loss)), double_layer_epoch_valid_loss, label=\"double-layer-valid\")\n",
    "plt.ylabel(\"Training mean squared error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training mean squared error\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(train_accs)), train_accs, label=\"ann-training-accuracy\")\n",
    "plt.plot(range(len(val_accs)), val_accs, label=\"ann-validation-accuracy\")\n",
    "# single-layer\n",
    "plt.plot(single_layer_epoch_train_acc, label=\"single-layer train\")\n",
    "plt.plot(single_layer_epoch_valid_acc, label=\"single-layer valid\")\n",
    "\n",
    "# double-layer\n",
    "plt.plot(double_layer_epoch_train_acc, label=\"double-layer train\")\n",
    "plt.plot(double_layer_epoch_valid_acc, label=\"double-layer valid\")\n",
    "\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
