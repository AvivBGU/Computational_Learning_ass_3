{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Machine Learning with PyTorch and Scikit-Learn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHANGES ####\n",
    "Moved imports to a dedicated cell.\n",
    "Moved magic nubmers to a dedicated constants cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED: int = 123\n",
    "NUM_EPOCHS: int = 50\n",
    "LEARNING_RATE: float = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and preparing the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset is publicly available at http://yann.lecun.com/exdb/mnist/ and consists of the following four parts:\n",
    "\n",
    "- Training set images: train-images-idx3-ubyte.gz (9.9 MB, 47 MB unzipped, 60,000 examples)\n",
    "- Training set labels: train-labels-idx1-ubyte.gz (29 KB, 60 KB unzipped, 60,000 labels)\n",
    "- Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 7.8 MB, 10,000 examples)\n",
    "- Test set labels: t10k-labels-idx1-ubyte.gz (5 KB, 10 KB unzipped, 10,000 labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = X.values\n",
    "y = y.astype(int).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize to [-1, 1] range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ((X / 255.) - .5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=10000, random_state=123, stratify=y)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_temp, y_temp, test_size=5000, random_state=123, stratify=y_temp)\n",
    "\n",
    "\n",
    "# optional to free up some memory by deleting non-used arrays:\n",
    "del X_temp, y_temp, X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model was changed to be modular to support an arbitrary number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "def sigmoid(z):                                        \n",
    "    return 1. / (1. + np.exp(-z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def int_to_onehot(y, num_labels):\n",
    "\n",
    "    ary = np.zeros((y.shape[0], num_labels))\n",
    "    for index, val in enumerate(y):\n",
    "        ary[index, val] = 1\n",
    "\n",
    "    return ary\n",
    "\n",
    "class NeuralNetMLP:\n",
    "    def __init__(self, \n",
    "                 num_features: int, \n",
    "                 hidden_layers_sizes: list[int],\n",
    "                 num_classes: int,\n",
    "                 random_seed=RANDOM_SEED\n",
    "                 ):\n",
    "        \n",
    "        \n",
    "        if len(hidden_layers_sizes) < 1:\n",
    "            raise ValueError(\"hidden_layers_sizes must contain at least one layer.\")\n",
    "        rng = np.random.RandomState(random_seed)\n",
    "        self.num_classes = num_classes\n",
    "        self.layer_weights = [rng.normal(loc=0.0, scale=0.1, size=(hidden_layers_sizes[0], num_features))]\n",
    "        self.biases = list()\n",
    "        # hidden\n",
    "        \n",
    "        \n",
    "        for index in range(1, len(hidden_layers_sizes)):\n",
    "            self.layer_weights.append(\n",
    "                rng.normal(\n",
    "                    loc=0.0, \n",
    "                    scale=0.1, \n",
    "                    size=(hidden_layers_sizes[index], hidden_layers_sizes[index-1])\n",
    "                )\n",
    "            )\n",
    "        for index in range(len(hidden_layers_sizes)):\n",
    "            self.biases.append(np.zeros(hidden_layers_sizes[index]))\n",
    "\n",
    "        # output\n",
    "        self.layer_weights.append(rng.normal(loc=0.0, scale=0.1, size=(num_classes, hidden_layers_sizes[-1])))\n",
    "        self.biases.append(np.zeros(num_classes))\n",
    "        \n",
    "    def forward(self, input: np.ndarray):\n",
    "        layer_activations = []\n",
    "        layer_pre_activations = []  # store z for ReLU derivative\n",
    "        current_input = input\n",
    "\n",
    "        num_of_layers = len(self.layer_weights)\n",
    "        for layer_index in range(num_of_layers):\n",
    "            z = np.dot(current_input, self.layer_weights[layer_index].T) + self.biases[layer_index]\n",
    "            layer_pre_activations.append(z)\n",
    "\n",
    "            # ReLU for hidden layers, sigmoid only at output\n",
    "            if num_of_layers == 2: # Added a special case for single layer for reproducibility\n",
    "                a = sigmoid(z)\n",
    "            else:\n",
    "                if layer_index < num_of_layers - 1:\n",
    "                    a = relu(z)\n",
    "                else:\n",
    "                    a = sigmoid(z)\n",
    "\n",
    "            layer_activations.append(a)\n",
    "            current_input = a\n",
    "\n",
    "        return layer_activations, layer_pre_activations, layer_activations[-1]\n",
    "\n",
    "    def backward(self, input, layer_activations, layer_pre_activations, network_output, targets):\n",
    "        N = input.shape[0]\n",
    "        targets_onehot = int_to_onehot(targets, self.num_classes)\n",
    "\n",
    "        num_layers = len(self.layer_weights)\n",
    "        loss_weights_list = [None] * num_layers\n",
    "        loss_biases_list  = [None] * num_layers\n",
    "\n",
    "        # output delta (sigmoid + MSE)\n",
    "        a_out = network_output\n",
    "        d_loss__d_a = 2.0 * (a_out - targets_onehot) / N\n",
    "        z_out = layer_pre_activations[-1]\n",
    "        delta = d_loss__d_a * (sigmoid(z_out) * (1.0 - sigmoid(z_out)))  # == a_out*(1-a_out)\n",
    "        use_sigmoid_hidden = (num_layers == 2)  # special case for single layer\n",
    "    # backprop\n",
    "        for layer_idx in range(num_layers - 1, -1, -1):\n",
    "            a_prev = input if layer_idx == 0 else layer_activations[layer_idx - 1]\n",
    "\n",
    "            loss_weights_list[layer_idx] = np.dot(delta.T, a_prev)\n",
    "            loss_biases_list[layer_idx]  = np.sum(delta, axis=0)\n",
    "\n",
    "            if layer_idx > 0:\n",
    "                curr_W = self.layer_weights[layer_idx]      # (out, in)\n",
    "                d_loss__d_a_prev = np.dot(delta, curr_W)    # (N, in)\n",
    "\n",
    "                z_prev = layer_pre_activations[layer_idx - 1]\n",
    "\n",
    "                if use_sigmoid_hidden:\n",
    "                    # hidden activation is sigmoid -> derivative uses a_prev*(1-a_prev)\n",
    "                    a_prev_act = layer_activations[layer_idx - 1]\n",
    "                    delta = d_loss__d_a_prev * (a_prev_act * (1.0 - a_prev_act))\n",
    "                else:\n",
    "                    # hidden activation is ReLU\n",
    "                    delta = d_loss__d_a_prev * relu_derivative(z_prev)\n",
    "\n",
    "        return loss_weights_list, loss_biases_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_model = NeuralNetMLP(num_features=28*28,\n",
    "                     hidden_layers_sizes=[50, 50],\n",
    "                     num_classes=10)\n",
    "single_layer_model = NeuralNetMLP(num_features=28*28,\n",
    "                     hidden_layers_sizes=[50],\n",
    "                     num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding the neural network training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "minibatch_size = 100\n",
    "\n",
    "\n",
    "def minibatch_generator(X, y, minibatch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, indices.shape[0] - minibatch_size \n",
    "                           + 1, minibatch_size):\n",
    "        batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "        \n",
    "        yield X[batch_idx], y[batch_idx]\n",
    "\n",
    "        \n",
    "# iterate over training epochs\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # iterate over minibatches\n",
    "    minibatch_gen = minibatch_generator(\n",
    "        X_train, y_train, minibatch_size)\n",
    "    \n",
    "    for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "        break\n",
    "        \n",
    "    break\n",
    "print(X_train_mini.shape)\n",
    "print(y_train_mini.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to compute the loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(targets, probas, num_labels=10):\n",
    "    onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "    return np.mean((onehot_targets - probas)**2)\n",
    "\n",
    "\n",
    "def accuracy(targets, predicted_labels):\n",
    "    return np.mean(predicted_labels == targets) \n",
    "\n",
    "def evaluate_model(model, features, true_labels):\n",
    "    _, _, probas = model.forward(features)\n",
    "    mse = mse_loss(true_labels, probas)\n",
    "    predicted_labels = np.argmax(probas, axis=1)\n",
    "    acc = accuracy(true_labels, predicted_labels)\n",
    "    return acc, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, mse = evaluate_model(single_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')\n",
    "acc, mse = evaluate_model(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial validation MSE: {mse:.1f}')\n",
    "print(f'Initial validation accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_and_acc(nnet, X, y, num_labels=10, minibatch_size=100):\n",
    "    mse, correct_pred, num_examples = 0., 0, 0\n",
    "    minibatch_gen = minibatch_generator(X, y, minibatch_size)\n",
    "        \n",
    "    for i, (features, targets) in enumerate(minibatch_gen):\n",
    "\n",
    "        _, _, probas = nnet.forward(features)\n",
    "        predicted_labels = np.argmax(probas, axis=1)\n",
    "        \n",
    "        onehot_targets = int_to_onehot(targets, num_labels=num_labels)\n",
    "        loss = np.mean((onehot_targets - probas)**2)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "        \n",
    "        num_examples += targets.shape[0]\n",
    "        mse += loss\n",
    "\n",
    "    mse = mse/(i+1)\n",
    "    acc = correct_pred/num_examples\n",
    "    return mse, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse, acc = compute_mse_and_acc(single_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')\n",
    "mse, acc = compute_mse_and_acc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial valid MSE: {mse:.1f}')\n",
    "print(f'Initial valid accuracy: {acc*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(model, X, y):\n",
    "    _, _, probas = model.forward(X)\n",
    "    y_onehot = int_to_onehot(y, num_labels=model.num_classes)\n",
    "    macro_auc = roc_auc_score(y_onehot, probas, multi_class=\"ovr\", average=\"macro\")\n",
    "    return macro_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc1 = compute_auc(single_layer_model, X_valid, y_valid)\n",
    "auc2 = compute_auc(multi_layer_model, X_valid, y_valid)\n",
    "print('Evaluating single layer model...')\n",
    "print(f'Initial valid AUC1: {auc1:.1f}')\n",
    "print('Evaluating double layer model...')\n",
    "print(f'Initial valid AUC2: {auc2:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, y_train, X_valid, y_valid, num_epochs,\n",
    "          learning_rate=0.1):\n",
    "\n",
    "    epoch_loss = []\n",
    "    epoch_train_acc = []\n",
    "    epoch_valid_acc = []\n",
    "\n",
    "    for e in range(num_epochs):\n",
    "\n",
    "        minibatch_gen = minibatch_generator(X_train, y_train, minibatch_size)\n",
    "\n",
    "        for X_train_mini, y_train_mini in minibatch_gen:\n",
    "\n",
    "            #### Forward ####\n",
    "            activations_list, pre_activations, a_out = model.forward(X_train_mini)\n",
    "            #### Backward ####\n",
    "            weight_loss_list, bias_loss_list = model.backward(\n",
    "                X_train_mini,\n",
    "                activations_list,\n",
    "                pre_activations,\n",
    "                a_out,\n",
    "                y_train_mini\n",
    "            )\n",
    "\n",
    "            #### Update ####\n",
    "            for layer_idx in range(len(model.layer_weights)):\n",
    "                model.layer_weights[layer_idx] -= learning_rate * weight_loss_list[layer_idx]\n",
    "                model.biases[layer_idx]        -= learning_rate * bias_loss_list[layer_idx]\n",
    "\n",
    "\n",
    "        #### Epoch Logging ####\n",
    "        train_mse, train_acc = compute_mse_and_acc(model, X_train, y_train)\n",
    "        valid_mse, valid_acc = compute_mse_and_acc(model, X_valid, y_valid)\n",
    "\n",
    "        train_acc *= 100.0\n",
    "        valid_acc *= 100.0\n",
    "\n",
    "        epoch_train_acc.append(train_acc)\n",
    "        epoch_valid_acc.append(valid_acc)\n",
    "        epoch_loss.append(train_mse)\n",
    "\n",
    "        print(f'Epoch: {e+1:03d}/{num_epochs:03d} '\n",
    "              f'| Train MSE: {train_mse:.2f} '\n",
    "              f'| Train Acc: {train_acc:.2f}% '\n",
    "              f'| Valid Acc: {valid_acc:.2f}%')\n",
    "    return epoch_loss, epoch_train_acc, epoch_valid_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_SEED) # for the training set shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training single layer model...')\n",
    "single_layer_epoch_loss, single_layer_epoch_train_acc, single_layer_epoch_valid_acc = train(\n",
    "    single_layer_model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training double layer model...')\n",
    "double_layer_epoch_loss, double_layer_epoch_train_acc, double_layer_epoch_valid_acc = train(\n",
    "    multi_layer_model, X_train, y_train, X_valid, y_valid,\n",
    "    num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the neural network performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_loss)), epoch_loss)\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(epoch_train_acc)), epoch_train_acc,\n",
    "         label='Training')\n",
    "plt.plot(range(len(epoch_valid_acc)), epoch_valid_acc,\n",
    "         label='Validation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_layer_test_mse, single_layer_test_acc = compute_mse_and_acc(single_layer_model, X_test, y_test)\n",
    "double_layer_test_mse, double_layer_test_acc = compute_mse_and_acc(multi_layer_model, X_test, y_test)\n",
    "print(f'Test accuracy, single layer: {single_layer_test_acc*100:.2f}%')\n",
    "print(f'Test accuracy, double layer: {double_layer_test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot failure cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_subset = X_test[:1000, :]\n",
    "y_test_subset = y_test[:1000]\n",
    "\n",
    "_, _, probas = model.forward(X_test_subset)\n",
    "test_pred = np.argmax(probas, axis=1)\n",
    "\n",
    "misclassified_images = X_test_subset[y_test_subset != test_pred][:25]\n",
    "misclassified_labels = test_pred[y_test_subset != test_pred][:25]\n",
    "correct_labels = y_test_subset[y_test_subset != test_pred][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, \n",
    "                       sharex=True, sharey=True, figsize=(8, 8))\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    img = misclassified_images[i].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "    ax[i].set_title(f'{i+1}) '\n",
    "                    f'True: {correct_labels[i]}\\n'\n",
    "                    f' Predicted: {misclassified_labels[i]}')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('figures/11_09.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
